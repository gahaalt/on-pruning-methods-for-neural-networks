{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training functools\n",
    "\n",
    "This module provides custom training functions for tensorflow training. Those tends to be a tad faster than keras built-in counterparts, probably due to smaller overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "from collections import defaultdict\n",
    "from itertools import islice\n",
    "\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y, model):\n",
    "    assert isinstance(model, tf.keras.Model)\n",
    "    assert model.optimizer is not None, \"Model not compiled!\"\n",
    "    assert model.loss is not None, \"Model not compiled!\"\n",
    "    mixed_precision = isinstance(\n",
    "        model.optimizer, tf.keras.mixed_precision.experimental.LossScaleOptimizer\n",
    "    )\n",
    "    with tf.GradientTape() as tape:\n",
    "        outs = model(x, training=True)\n",
    "        outs = tf.cast(outs, tf.float32)\n",
    "        loss = model.compiled_loss(y, outs, regularization_losses=model.losses)\n",
    "        if mixed_precision:\n",
    "            loss = model.optimizer.get_scaled_loss(loss)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    if mixed_precision:\n",
    "        gradients = model.optimizer.get_unscaled_gradients(gradients)\n",
    "\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    model.compiled_metrics.update_state(y, outs)\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_step` is a lowest level function for training the model. Because it is decorated with `@tf.function`, `assert` statemets should not be executed during the runtime -- but only during the tracing. This is great, because it provides necessary testing without slowing down the runtime. This implementation provides mixed precision support -- also without slowing down the runtime in case if mixed precision is not used. Here is a minimal example of model training that demonstrates decaying loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from modules.tf_helper.models import VGG, WRN\n",
    "from modules.tf_helper.datasets import cifar\n",
    "from itertools import islice\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# model = VGG((32, 32, 3), 10, version=11)\n",
    "model = WRN(16, 8, (32, 32, 3), 10)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')],\n",
    ")\n",
    "data = cifar(shuffle_train=False)\n",
    "\n",
    "losses = []\n",
    "accurs = []\n",
    "for x, y in tqdm(islice(data['train'], 100), total=100):\n",
    "    outs = train_step(x, y, model)\n",
    "    losses.append(model.metrics[0].result())\n",
    "    accurs.append(model.metrics[1].result())\n",
    "    for metric in model.metrics:\n",
    "        metric.reset_states()\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.plot(range(len(losses)), losses)\n",
    "plt.subplot(212)\n",
    "plt.plot(range(len(accurs)), accurs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above we train the network for 100 iterations. The loss should start going down and accuracy should start raising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def train_epoch(iterator, model, epoch_idx=0, steps=None, callbacks=(), use_pbar=None):\n",
    "    for callback in callbacks:\n",
    "        assert isinstance(callback, tf.keras.callbacks.Callback)\n",
    "        callback.on_epoch_begin(epoch_idx)\n",
    "\n",
    "    if use_pbar:\n",
    "        pbar = use_pbar\n",
    "    else:\n",
    "        pbar = tqdm(total=steps, leave=True, ascii=True)\n",
    "\n",
    "    for bidx, (x, y) in enumerate(islice(iterator, steps)):\n",
    "        for callback in callbacks:\n",
    "            assert isinstance(callback, tf.keras.callbacks.Callback)\n",
    "            callback.on_train_batch_begin(bidx)\n",
    "\n",
    "        outs = train_step(x, y, model)\n",
    "        pbar.set_postfix(\n",
    "            {m.name: m.result().numpy() for m in model.metrics}, refresh=False\n",
    "        )\n",
    "\n",
    "        for callback in callbacks:\n",
    "            assert isinstance(callback, tf.keras.callbacks.Callback)\n",
    "            callback.on_train_batch_end(bidx)\n",
    "        pbar.update()\n",
    "\n",
    "    if not use_pbar:\n",
    "        pbar.close()\n",
    "\n",
    "    for callback in callbacks:\n",
    "        assert isinstance(callback, tf.keras.callbacks.Callback)\n",
    "        callback.on_epoch_end(epoch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "train_epoch(\n",
    "    data[\"train\"], model, steps=100, callbacks=[tf.keras.callbacks.Callback()]\n",
    ")\n",
    "print(f\"TIME: {time.time() - t0}\")\n",
    "\n",
    "for metric in model.metrics:\n",
    "    print(f\"{metric.name} = {metric.result()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "@tf.function\n",
    "def valid_step(x, y, model):\n",
    "    assert isinstance(model, tf.keras.Model)\n",
    "    assert model.loss is not None, \"Model not compiled!\"\n",
    "\n",
    "    outs = model(x, training=False)\n",
    "    outs = tf.cast(outs, tf.float32)\n",
    "    model.compiled_loss(y, outs)\n",
    "    model.compiled_metrics.update_state(y, outs)\n",
    "    return outs\n",
    "\n",
    "\n",
    "def valid_epoch(iterator, model, epoch_idx=0, steps=None, callbacks=()):\n",
    "    for bidx, (x, y) in enumerate(islice(iterator, steps)):\n",
    "        for callback in callbacks:\n",
    "            assert isinstance(callback, tf.keras.callbacks.Callback)\n",
    "            callback.on_test_batch_begin(bidx)\n",
    "\n",
    "        outs = valid_step(x, y, model)\n",
    "\n",
    "        for callback in callbacks:\n",
    "            assert isinstance(callback, tf.keras.callbacks.Callback)\n",
    "            callback.on_test_batch_end(bidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_epoch(data[\"test\"], model)\n",
    "for m in model.metrics:\n",
    "    print(f\"{m.name}: {m.result()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def reset_metrics(model):\n",
    "    results = {m.name: m.result().numpy() for m in model.metrics}\n",
    "    for metric in model.metrics:\n",
    "        metric.reset_states()\n",
    "    return results\n",
    "\n",
    "\n",
    "def fit(\n",
    "    model,\n",
    "    training_data,\n",
    "    validation_data,\n",
    "    steps_per_epoch=None,\n",
    "    epochs=1,\n",
    "    initial_epoch=0,\n",
    "    callbacks=(),\n",
    "):\n",
    "    history = defaultdict(list)\n",
    "\n",
    "    bpbar = tqdm(total=epochs, leave=True, ascii=True)\n",
    "    for epoch_idx in range(initial_epoch, epochs):\n",
    "        pbar = tqdm(total=steps_per_epoch, leave=True, ascii=True)\n",
    "        train_epoch(\n",
    "            training_data,\n",
    "            model,\n",
    "            epoch_idx=epoch_idx,\n",
    "            steps=steps_per_epoch,\n",
    "            callbacks=callbacks,\n",
    "            use_pbar=pbar,\n",
    "        )\n",
    "        metrics = reset_metrics(model)\n",
    "        for key, value in metrics.items():\n",
    "            history[key].append(value)\n",
    "\n",
    "        valid_epoch(\n",
    "            validation_data,\n",
    "            model,\n",
    "            epoch_idx=epoch_idx,\n",
    "            steps=steps_per_epoch,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "        metrics = reset_metrics(model)\n",
    "        for key, value in metrics.items():\n",
    "            history[\"val_\" + key].append(value)\n",
    "\n",
    "        metrics = reset_metrics(model)\n",
    "\n",
    "        pbar.close()\n",
    "        bpbar.set_postfix({key: value[-1] for key, value in history.items()})\n",
    "        bpbar.update()\n",
    "    return dict(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, data[\"train\"], data[\"test\"], steps_per_epoch=10, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run convert_notebooks.py"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "55e71a62cc267b8a0bae4b336a38f88abe99825cae755cc76843f5de83cc683c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
